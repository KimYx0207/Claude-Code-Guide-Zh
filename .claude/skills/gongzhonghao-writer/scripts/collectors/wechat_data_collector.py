# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ•°æ®æ”¶é›†å™¨
ç”¨äºæ”¶é›†å…¬ä¼—å·åå°æ–‡ç« æ•°æ®ï¼ˆé˜…è¯»æ•°ã€ç‚¹èµã€åœ¨çœ‹ç­‰ï¼‰

ä½œè€…ï¼šè€é‡‘
æ—¥æœŸï¼š2025-11-29
"""

import json
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional


# è·å–é¡¹ç›®æ ¹ç›®å½•ï¼šä»è„šæœ¬ä½ç½®å‘ä¸Š4çº§ï¼ˆscripts -> prompts -> gongzhonghao-writer -> skills -> .claude -> é¡¹ç›®æ ¹ï¼‰
# æˆ–ä½¿ç”¨ç¯å¢ƒå˜é‡CLAUDE_PROJECT_DIR
PROJECT_ROOT = Path(os.getenv('CLAUDE_PROJECT_DIR', Path(__file__).parent.parent.parent.parent.parent))
DEFAULT_DATA_DIR = PROJECT_ROOT / "data"


class WeChatDataCollector:
    """å¾®ä¿¡å…¬ä¼—å·æ•°æ®æ”¶é›†å™¨"""

    def __init__(self, data_dir: Path = None):
        """
        åˆå§‹åŒ–æ•°æ®æ”¶é›†å™¨

        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•ï¼ˆPathå¯¹è±¡æˆ–Noneä½¿ç”¨é»˜è®¤ï¼‰
                     ä¿®å¤ï¼šä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œé¿å…ç›¸å¯¹è·¯å¾„"data"å¯¼è‡´çš„è·¯å¾„é”™è¯¯
        """
        # ä½¿ç”¨ä¼ å…¥çš„data_dirï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤çš„é¡¹ç›®æ ¹/data
        if data_dir is None:
            data_dir = DEFAULT_DATA_DIR
        elif isinstance(data_dir, str):
            # å‘åå…¼å®¹ï¼šå¦‚æœä¼ å…¥å­—ç¬¦ä¸²ï¼Œè½¬ä¸ºPathå¹¶è§£æä¸ºç»å¯¹è·¯å¾„
            data_dir = Path(data_dir)
            if not data_dir.is_absolute():
                data_dir = PROJECT_ROOT / data_dir

        self.data_dir = data_dir
        self.data_dir.mkdir(exist_ok=True, parents=True)
        self.data_file = self.data_dir / "wechat_articles.json"
        self.history_file = self.data_dir / "wechat_history.json"

    def parse_simple_text(self, text: str) -> List[Dict]:
        """
        è§£æç®€åŒ–æ–‡æœ¬æ ¼å¼ï¼ˆç”¨æˆ·æ‰‹åŠ¨å¤åˆ¶çš„æ•°æ®ï¼‰

        æ ¼å¼ç¤ºä¾‹:
        ä»Šå¤© 17:29
        å·²å‘è¡¨
        é¦™æ¸¯2äº¿AIè¯ˆéª—æ¡ˆç»†èŠ‚æ›å…‰ï¼Œè§†é¢‘ä¼šè®®é‡Œåªæœ‰ä½ æ˜¯çœŸäºº
        åŸåˆ›
        673 13 48 5 3 4 Â¥0.00 0

        Args:
            text: ç®€åŒ–æ–‡æœ¬

        Returns:
            æ–‡ç« æ•°æ®åˆ—è¡¨
        """
        articles = []
        lines = [line.strip() for line in text.split('\n') if line.strip()]

        i = 0
        while i < len(lines):
            current_article = {}

            # 1. å‘å¸ƒæ—¶é—´
            if i < len(lines) and re.match(r'^(ä»Šå¤©|æ˜¨å¤©|æ˜ŸæœŸ[ä¸€äºŒä¸‰å››äº”å…­æ—¥]|[\d]+æœˆ[\d]+æ—¥)(\s+\d+:\d+)?$', lines[i]):
                current_article['publish_time'] = lines[i]
                i += 1
            else:
                i += 1
                continue

            # 2. "å·²å‘è¡¨" æ ‡è®°ï¼ˆè·³è¿‡ï¼‰
            if i < len(lines) and lines[i] == 'å·²å‘è¡¨':
                i += 1

            # 3. æ–‡ç« æ ‡é¢˜
            if i < len(lines) and len(lines[i]) > 10 and lines[i] != 'åŸåˆ›':
                current_article['title'] = lines[i]
                i += 1
            else:
                i += 1
                continue

            # 4. "åŸåˆ›" æ ‡è®°ï¼ˆè·³è¿‡ï¼‰
            if i < len(lines) and lines[i] == 'åŸåˆ›':
                i += 1

            # 5. æ•°æ®è¡Œ: "673 13 48 5 3 4 Â¥0.00 0"
            if i < len(lines):
                parts = lines[i].split()
                if len(parts) >= 3:
                    try:
                        current_article['read_count'] = int(parts[0].replace(',', ''))
                        current_article['like_count'] = int(parts[1].replace(',', ''))
                        current_article['look_count'] = int(parts[2].replace(',', ''))
                        current_article['comment_count'] = int(parts[3].replace(',', '')) if len(parts) > 3 else 0
                        current_article['share_count'] = int(parts[4].replace(',', '')) if len(parts) > 4 else 0
                        current_article['underline_count'] = int(parts[5].replace(',', '')) if len(parts) > 5 else 0
                        current_article['reward_amount'] = parts[6] if len(parts) > 6 else 'Â¥0.00'
                    except (ValueError, IndexError):
                        pass
                i += 1

            # æ·»åŠ æ–‡ç« ï¼ˆå¿…é¡»æœ‰æ ‡é¢˜å’Œå‘å¸ƒæ—¶é—´ï¼‰
            if 'title' in current_article and 'publish_time' in current_article:
                # è®¾ç½®é»˜è®¤å€¼
                current_article.setdefault('read_count', 0)
                current_article.setdefault('like_count', 0)
                current_article.setdefault('look_count', 0)
                current_article.setdefault('comment_count', 0)
                current_article.setdefault('share_count', 0)
                current_article.setdefault('underline_count', 0)
                current_article.setdefault('reward_amount', 'Â¥0.00')
                current_article.setdefault('url', '')

                articles.append(current_article)

            # è·³è¿‡ç©ºè¡Œ
            while i < len(lines) and not lines[i]:
                i += 1

        return articles

    def parse_snapshot_text(self, snapshot_text: str) -> List[Dict]:
        """
        ä»Playwright snapshotæ–‡æœ¬ä¸­è§£ææ–‡ç« æ•°æ®

        Args:
            snapshot_text: snapshotæ–‡æœ¬å†…å®¹

        Returns:
            æ–‡ç« æ•°æ®åˆ—è¡¨
        """
        articles = []
        lines = snapshot_text.split('\n')

        # ä¸´æ—¶å­˜å‚¨å½“å‰æ–‡ç« æ•°æ®
        current_article = {}

        # è®°å½•æ˜¯å¦åœ¨æ–‡ç« åŒºåŸŸï¼ˆå·²å‘è¡¨ï¼‰
        in_article_section = False

        for line in lines:
            # æå–StaticTextå†…å®¹
            static_text_match = re.search(r'StaticText "([^"]+)"', line)
            if not static_text_match:
                continue

            text_content = static_text_match.group(1)

            # æå‰æ£€æµ‹å‘å¸ƒæ—¶é—´ï¼ˆåŒ…æ‹¬ç¬¬ä¸€ç¯‡åœ¨"å·²å‘è¡¨"ä¹‹å‰çš„æ–‡ç« ï¼‰
            time_match = re.match(r'^(ä»Šå¤©|æ˜¨å¤©|æ˜ŸæœŸ[ä¸€äºŒä¸‰å››äº”å…­æ—¥]|[\d]+æœˆ[\d]+æ—¥)(\s+\d+:\d+)?$', text_content)
            if time_match:
                # ä¿å­˜å‰ä¸€ç¯‡æ–‡ç« ï¼ˆåªè¦æœ‰æ—¶é—´å°±ç®—ä¸€ç¯‡æ–‡ç« ï¼‰
                if current_article and 'publish_time' in current_article:
                    articles.append(current_article)
                # å¼€å§‹æ–°æ–‡ç« ï¼Œå¹¶è‡ªåŠ¨è¿›å…¥æ–‡ç« åŒºåŸŸ
                current_article = {'publish_time': text_content}
                in_article_section = True
                continue

            # æ£€æµ‹æ˜¯å¦è¿›å…¥æ–‡ç« åŒºåŸŸ
            if text_content == "å·²å‘è¡¨":
                in_article_section = True
                continue

            if not in_article_section:
                continue

            # æå–æ–‡ç« æ ‡é¢˜å’ŒURL
            # æ ‡é¢˜ï¼šé•¿æ–‡æœ¬ï¼ˆ>10å­—ç¬¦ï¼‰ï¼Œä¸æ˜¯"åŸåˆ›"ï¼Œä¸æ˜¯ç©ºæ ¼ï¼Œä¸åŒ…å«æ•°å­—
            if text_content != 'åŸåˆ›' and len(text_content) > 10 and not re.match(r'^[\s\u200b]+$', text_content):
                # æ£€æŸ¥æ˜¯å¦æ˜¯çº¯æ•°å­—æˆ–æ ‡ç‚¹
                if not re.match(r'^[\d,.\sÂ¥]+$', text_content):
                    # å¦‚æœå½“å‰æ–‡ç« è¿˜æ²¡æœ‰æ ‡é¢˜ï¼Œè¿™å°±æ˜¯æ ‡é¢˜
                    if 'title' not in current_article:
                        current_article['title'] = text_content
                        # å°è¯•ä»å½“å‰è¡Œæˆ–å‰ä¸€è¡Œæå–URL
                        url_match = re.search(r'url="(https://mp\.weixin\.qq\.com/s/[^"]+)"', line)
                        if url_match:
                            current_article['url'] = url_match.group(1)
                continue

            # æå–æ•°å­—æ•°æ®ï¼ˆé˜…è¯»ã€ç‚¹èµã€åœ¨çœ‹ã€è¯„è®ºã€è½¬å‘ï¼‰
            # å¿…é¡»æ˜¯çº¯æ•°å­—ï¼ˆå¯èƒ½åŒ…å«é€—å·ï¼Œå¦‚"1,079"ï¼‰
            num_match = re.match(r'^([\d,]+)$', text_content)
            if num_match:
                # ç§»é™¤é€—å·
                num_str = num_match.group(1).replace(',', '')
                num = int(num_str)

                # æŒ‰é¡ºåºè¯†åˆ«ï¼šé˜…è¯»ã€ç‚¹èµã€åœ¨çœ‹ã€è¯„è®ºã€è½¬å‘
                if 'read_count' not in current_article:
                    current_article['read_count'] = num
                elif 'like_count' not in current_article:
                    current_article['like_count'] = num
                elif 'look_count' not in current_article:
                    current_article['look_count'] = num
                elif 'comment_count' not in current_article:
                    current_article['comment_count'] = num
                elif 'share_count' not in current_article:
                    current_article['share_count'] = num
                continue

            # æå–åˆ’çº¿æ•°ï¼ˆåœ¨mpunderlineé“¾æ¥ä¸­ï¼‰
            if 'mpunderline' in line:
                underline_match = re.match(r'^\d+$', text_content)
                if underline_match:
                    current_article['underline_count'] = int(text_content)
                continue

            # æå–èµèµé‡‘é¢
            if 'Â¥' in text_content:
                reward_match = re.search(r'Â¥([\d.]+)', text_content)
                if reward_match:
                    current_article['reward_amount'] = float(reward_match.group(1))
                continue

        # ä¿å­˜æœ€åä¸€ç¯‡æ–‡ç« 
        if current_article and 'publish_time' in current_article:
            articles.append(current_article)

        return articles

    def load_existing_data(self) -> List[Dict]:
        """
        åŠ è½½å·²å­˜åœ¨çš„æ•°æ®

        Returns:
            ç°æœ‰æ–‡ç« æ•°æ®åˆ—è¡¨
        """
        if self.data_file.exists():
            with open(self.data_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []

    def save_data(self, articles: List[Dict]):
        """
        ä¿å­˜æ–‡ç« æ•°æ®

        Args:
            articles: æ–‡ç« æ•°æ®åˆ—è¡¨
        """
        with open(self.data_file, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)

    def save_history_snapshot(self, articles: List[Dict], snapshot_date: str):
        """
        ä¿å­˜å†å²å¿«ç…§

        Args:
            articles: æ–‡ç« æ•°æ®åˆ—è¡¨
            snapshot_date: å¿«ç…§æ—¥æœŸ
        """
        history = {}
        if self.history_file.exists():
            with open(self.history_file, 'r', encoding='utf-8') as f:
                history = json.load(f)

        history[snapshot_date] = articles

        with open(self.history_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)

    def merge_incremental_data(self, new_articles: List[Dict]) -> Dict:
        """
        å¢é‡åˆå¹¶æ•°æ®

        Args:
            new_articles: æ–°çˆ¬å–çš„æ–‡ç« åˆ—è¡¨

        Returns:
            åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
        """
        existing = self.load_existing_data()
        existing_titles = {article.get('title') for article in existing if 'title' in article}

        # æ‰¾å‡ºæ–°å¢æ–‡ç« 
        new_count = 0
        updated_count = 0
        skipped_count = 0

        for article in new_articles:
            # è·³è¿‡æ²¡æœ‰titleçš„æ–‡ç« 
            if 'title' not in article:
                skipped_count += 1
                continue

            if article['title'] not in existing_titles:
                existing.append(article)
                new_count += 1
            else:
                # æ›´æ–°å·²å­˜åœ¨æ–‡ç« çš„æ•°æ®
                for i, old_article in enumerate(existing):
                    if old_article.get('title') == article['title']:
                        existing[i] = article
                        updated_count += 1
                        break

        # ä¿å­˜åˆå¹¶åçš„æ•°æ®
        self.save_data(existing)

        # ä¿å­˜å†å²å¿«ç…§
        today = datetime.now().strftime("%Y-%m-%d")
        self.save_history_snapshot(existing, today)

        return {
            'total': len(existing),
            'new': new_count,
            'updated': updated_count
        }

    def generate_report(self) -> str:
        """
        ç”Ÿæˆæ•°æ®æ”¶é›†æŠ¥å‘Š

        Returns:
            æŠ¥å‘Šæ–‡æœ¬
        """
        articles = self.load_existing_data()

        if not articles:
            return "[ERROR] æš‚æ— æ•°æ®"

        # ç»Ÿè®¡æ•°æ®
        total_read = sum(a.get('read_count', 0) for a in articles)
        total_like = sum(a.get('like_count', 0) for a in articles)
        total_look = sum(a.get('look_count', 0) for a in articles)
        avg_read = total_read // len(articles) if articles else 0

        # æ‰¾å‡ºçˆ†æ¬¾æ–‡ç« ï¼ˆé˜…è¯»æ•°>1000ï¼‰
        hot_articles = [a for a in articles if a.get('read_count', 0) > 1000]
        hot_articles.sort(key=lambda x: x.get('read_count', 0), reverse=True)

        report = f"""
========== å¾®ä¿¡å…¬ä¼—å·æ•°æ®æ”¶é›†æŠ¥å‘Š ==========
{'='*50}

[æ€»ä½“ç»Ÿè®¡]
- æ–‡ç« æ€»æ•°ï¼š{len(articles)}ç¯‡
- æ€»é˜…è¯»æ•°ï¼š{total_read:,}
- æ€»ç‚¹èµæ•°ï¼š{total_like}
- æ€»åœ¨çœ‹æ•°ï¼š{total_look}
- å¹³å‡é˜…è¯»ï¼š{avg_read:,}/ç¯‡

[çˆ†æ¬¾æ–‡ç« ï¼ˆé˜…è¯»>1000ï¼‰]
"""

        for i, article in enumerate(hot_articles[:10], 1):
            report += f"\n{i}ã€{article['title']}"
            report += f"\n   é˜…è¯»ï¼š{article.get('read_count', 0):,} | ç‚¹èµï¼š{article.get('like_count', 0)} | åœ¨çœ‹ï¼š{article.get('look_count', 0)}"

        return report

    def generate_incremental_report(self, new_articles: List[Dict], skipped_count: int, pages_collected: int) -> str:
        """
        ç”Ÿæˆå¢é‡æ”¶é›†æŠ¥å‘Šï¼ˆåŒ…å«å¢é‡ä¿¡æ¯å’Œæ€»ä½“ç»Ÿè®¡ï¼‰

        Args:
            new_articles: æ–°å¢æ–‡ç« åˆ—è¡¨
            skipped_count: è·³è¿‡çš„æ–‡ç« æ•°ï¼ˆé‡å¤ï¼‰
            pages_collected: æ”¶é›†çš„é¡µæ•°

        Returns:
            å¢é‡æŠ¥å‘Šæ–‡æœ¬
        """
        all_articles = self.load_existing_data()

        if not all_articles:
            return "[ERROR] æš‚æ— æ•°æ®"

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_read = sum(a.get('read_count', 0) for a in all_articles)
        total_like = sum(a.get('like_count', 0) for a in all_articles)
        total_look = sum(a.get('look_count', 0) for a in all_articles)
        avg_read = total_read // len(all_articles) if all_articles else 0

        # è®¡ç®—æ–°å¢æ–‡ç« çš„ç»Ÿè®¡
        new_read = sum(a.get('read_count', 0) for a in new_articles)
        new_like = sum(a.get('like_count', 0) for a in new_articles)
        new_look = sum(a.get('look_count', 0) for a in new_articles)

        # æ‰¾å‡ºçˆ†æ¬¾æ–‡ç« 
        hot_articles = [a for a in all_articles if a.get('read_count', 0) > 1000]
        hot_articles.sort(key=lambda x: x.get('read_count', 0), reverse=True)

        # ç”ŸæˆæŠ¥å‘Š
        report = f"""
========== å¾®ä¿¡å…¬ä¼—å·å¢é‡æ•°æ®æ”¶é›†æŠ¥å‘Š ==========
{'='*50}

[æ”¶é›†æ¦‚å†µ]
- æ”¶é›†æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- æ”¶é›†é¡µæ•°ï¼š{pages_collected}é¡µï¼ˆæ™ºèƒ½åœæ­¢ï¼‰
- åœæ­¢åŸå› ï¼š{'æ£€æµ‹åˆ°é‡å¤æ–‡ç« ' if skipped_count > 0 else 'å®Œæˆæ”¶é›†'}

[å¢é‡ç»Ÿè®¡]
- æ–°å¢æ–‡ç« ï¼š{len(new_articles)}ç¯‡
- è·³è¿‡æ–‡ç« ï¼š{skipped_count}ç¯‡ï¼ˆé‡å¤ï¼‰
- æ–°å¢é˜…è¯»ï¼š+{new_read:,}
- æ–°å¢ç‚¹èµï¼š+{new_like}
- æ–°å¢åœ¨çœ‹ï¼š+{new_look}

[æ€»ä½“ç»Ÿè®¡]
- æ–‡ç« æ€»æ•°ï¼š{len(all_articles)}ç¯‡
- æ€»é˜…è¯»æ•°ï¼š{total_read:,}
- æ€»ç‚¹èµæ•°ï¼š{total_like}
- æ€»åœ¨çœ‹æ•°ï¼š{total_look}
- å¹³å‡é˜…è¯»ï¼š{avg_read:,}/ç¯‡

[æ–°å¢æ–‡ç« åˆ—è¡¨]
"""

        for i, article in enumerate(new_articles, 1):
            report += f"\n{i}ã€{article['title']}ï¼ˆ{article.get('publish_time', 'æœªçŸ¥')}ï¼‰"
            report += f"\n   é˜…è¯»{article.get('read_count', 0)} | ç‚¹èµ{article.get('like_count', 0)} | åœ¨çœ‹{article.get('look_count', 0)}"

        report += "\n\n[çˆ†æ¬¾æ–‡ç«  TOP 10]"

        for i, article in enumerate(hot_articles[:10], 1):
            report += f"\n\n{i}ã€{article['title']}"
            report += f"\n   é˜…è¯»ï¼š{article.get('read_count', 0):,} | ç‚¹èµï¼š{article.get('like_count', 0)} | åœ¨çœ‹ï¼š{article.get('look_count', 0)}"

        return report


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    collector = WeChatDataCollector()
    print("âœ… æ•°æ®æ”¶é›†å™¨åˆå§‹åŒ–æˆåŠŸ")
    print(f"ğŸ“ æ•°æ®ç›®å½•ï¼š{collector.data_dir.absolute()}")
