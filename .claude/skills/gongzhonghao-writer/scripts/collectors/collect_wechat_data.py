# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ•°æ®æ”¶é›†ä¸»è„šæœ¬
é€šè¿‡Playwright MCPæ§åˆ¶æµè§ˆå™¨ï¼Œè‡ªåŠ¨ç¿»é¡µæ”¶é›†æ–‡ç« æ•°æ®

ä½œè€…ï¼šè€é‡‘
æ—¥æœŸï¼š2025-11-29

é‡è¦è¯´æ˜ï¼š
- æ­¤è„šæœ¬è®¾è®¡ä¸ºç”±Slashå‘½ä»¤ `/collect-wechat-data` è°ƒç”¨
- ä¾èµ–å¤–éƒ¨MCPå·¥å…·ï¼ˆPlaywright MCPï¼‰æä¾›snapshot_provider
- ä¸èƒ½ç‹¬ç«‹è¿è¡Œï¼Œéœ€è¦é€šè¿‡Claude Codeçš„å·¥å…·è°ƒç”¨
"""

import json
import time
from pathlib import Path
from typing import Dict, List, Callable
from wechat_data_collector import WeChatDataCollector


class WeChatDataCrawler:
    """å¾®ä¿¡å…¬ä¼—å·æ•°æ®çˆ¬è™« - é€šè¿‡MCPæ§åˆ¶æµè§ˆå™¨"""

    def __init__(self, base_url: str, total_pages: int = 17):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            base_url: å¾®ä¿¡å…¬ä¼—å·åå°åŸºç¡€URLï¼ˆåŒ…å«tokenï¼‰
            total_pages: è¦çˆ¬å–çš„æ€»é¡µæ•°
        """
        self.base_url = base_url
        self.total_pages = total_pages
        self.collector = WeChatDataCollector()

    def generate_page_url(self, page_num: int) -> str:
        """
        ç”ŸæˆæŒ‡å®šé¡µç çš„URL

        Args:
            page_num: é¡µç ï¼ˆä»1å¼€å§‹ï¼‰

        Returns:
            å®Œæ•´URL
        """
        begin = (page_num - 1) * 10
        # å‡è®¾base_urlæ ¼å¼: https://mp.weixin.qq.com/cgi-bin/appmsgpublish?sub=list&begin=0&count=10&token=XXX&lang=zh_CN
        # æ›¿æ¢beginå‚æ•°
        import re
        return re.sub(r'begin=\d+', f'begin={begin}', self.base_url)

    def collect_all_pages(self, snapshot_provider: Callable[[int], str]) -> Dict:
        """
        æ”¶é›†æ‰€æœ‰é¡µé¢æ•°æ®

        Args:
            snapshot_provider: ä¸€ä¸ªå‡½æ•°ï¼Œæ¥å—URLï¼Œè¿”å›snapshotæ–‡æœ¬
                              ä¾‹å¦‚: lambda url: mcp__Playwright__take_snapshot()

        Returns:
            æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        """
        all_articles = []

        print(f"ğŸš€ å¼€å§‹æ”¶é›†æ•°æ®ï¼Œå…±{self.total_pages}é¡µ")

        for page_num in range(1, self.total_pages + 1):
            print(f"\nğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {page_num}/{self.total_pages} é¡µ...")

            # ä¸éœ€è¦å¯¼èˆªåˆ°ç¬¬1é¡µï¼ˆç”¨æˆ·å·²ç»åœ¨é‚£é‡Œäº†ï¼‰
            if page_num > 1:
                url = self.generate_page_url(page_num)
                print(f"   å¯¼èˆªåˆ°: {url}")
                # è¿™é‡Œéœ€è¦è°ƒç”¨MCPçš„navigateå‘½ä»¤
                # ç”±äºè¿™æ˜¯Pythonè„šæœ¬ï¼Œå®é™…å¯¼èˆªéœ€è¦åœ¨å¤–éƒ¨å®Œæˆ
                # è¿™ä¸ªå‡½æ•°åº”è¯¥ç”±è°ƒç”¨è€…æä¾›snapshot
                pass

            # è·å–å½“å‰é¡µé¢çš„snapshot
            try:
                snapshot_text = snapshot_provider(page_num)

                # è§£ææ–‡ç« æ•°æ®
                articles = self.collector.parse_snapshot_text(snapshot_text)
                print(f"   âœ… æå–åˆ° {len(articles)} ç¯‡æ–‡ç« ")

                all_articles.extend(articles)

                # é¿å…è¯·æ±‚è¿‡å¿«
                if page_num < self.total_pages:
                    time.sleep(1.5)

            except Exception as e:
                print(f"   âŒ ç¬¬{page_num}é¡µå¤„ç†å¤±è´¥: {str(e)}")
                continue

        # å¢é‡åˆå¹¶æ•°æ®
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®...")
        stats = self.collector.merge_incremental_data(all_articles)

        print(f"\nâœ… æ•°æ®æ”¶é›†å®Œæˆï¼")
        print(f"   - æ€»æ–‡ç« æ•°: {stats['total']}")
        print(f"   - æ–°å¢æ–‡ç« : {stats['new']}")
        print(f"   - æ›´æ–°æ–‡ç« : {stats['updated']}")

        return stats

    def generate_report(self) -> str:
        """
        ç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Š

        Returns:
            æŠ¥å‘Šæ–‡æœ¬
        """
        return self.collector.generate_report()


def main():
    """ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•"""
    # è¿™ä¸ªè„šæœ¬éœ€è¦é…åˆMCPå‘½ä»¤ä½¿ç”¨
    # å®é™…æ‰§è¡Œæ—¶åº”è¯¥ç”±Slashå‘½ä»¤è°ƒç”¨
    print("âŒ æ­¤è„šæœ¬éœ€è¦é€šè¿‡ /collect-wechat-data å‘½ä»¤è°ƒç”¨")
    print("   ä¸èƒ½ç›´æ¥è¿è¡Œï¼Œå› ä¸ºéœ€è¦Playwright MCPæä¾›snapshot")


if __name__ == "__main__":
    main()
