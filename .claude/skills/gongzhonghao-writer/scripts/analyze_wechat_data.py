#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ•°æ®åˆ†æè„šæœ¬ V7.0 - ç§‘å­¦ç»Ÿè®¡ç‰ˆ
ä½¿ç”¨ä¸“ä¸šç»Ÿè®¡æ–¹æ³•åˆ†æå…¬ä¼—å·æ–‡ç« è¡¨ç°

ä¸»è¦æ”¹è¿›ï¼š
- IQRæ–¹æ³•å®šä¹‰çˆ†æ¬¾é˜ˆå€¼ï¼ˆæ›¿ä»£ç¡¬ç¼–ç 1000ï¼‰
- TF-IDFå…³é”®è¯æå–ï¼ˆæ›¿ä»£å›ºå®šå…³é”®è¯åˆ—è¡¨ï¼‰
- ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒï¼ˆt-testã€p-valueï¼‰
- 95%ç½®ä¿¡åŒºé—´å’Œæ ‡å‡†è¯¯
- å¤šæ ‡ç­¾è¯é¢˜åˆ†ç±»
- æ”¹è¿›çš„æ—¶é—´åºåˆ—åˆ†æ

ä½œè€…ï¼šè€é‡‘
æ—¥æœŸï¼š2025-12-02
"""

import json
import sys
import io
from pathlib import Path

# å¼ºåˆ¶UTF-8è¾“å‡ºï¼ˆWindowså…¼å®¹æ€§ä¿®å¤ï¼‰
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
from collections import Counter, defaultdict
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Any
import re

# ç»Ÿè®¡åˆ†æåº“
import numpy as np
from scipy import stats
from scipy.stats import ttest_ind

# TF-IDFåº“
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba
import jieba.analyse


class WeChatDataAnalyzer:
    """å¾®ä¿¡å…¬ä¼—å·æ•°æ®åˆ†æå™¨ - ç§‘å­¦ç»Ÿè®¡ç‰ˆ"""

    def __init__(self, data_file: str):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            data_file: wechat_articles.jsonæ–‡ä»¶è·¯å¾„
        """
        self.data_file = Path(data_file)
        self.articles: List[Dict[str, Any]] = []
        self.analysis_results: Dict[str, Any] = {}

        # æœ€å°æ ·æœ¬é‡æ£€æŸ¥
        self.MIN_SAMPLE_SIZE = 30  # ç»Ÿè®¡å­¦å»ºè®®æœ€å°æ ·æœ¬é‡

    def load_data(self) -> bool:
        """
        åŠ è½½æ•°æ®æ–‡ä»¶

        Returns:
            åŠ è½½æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        if not self.data_file.exists():
            print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {self.data_file}")
            return False

        try:
            with open(self.data_file, 'r', encoding='utf-8') as f:
                self.articles = json.load(f)

            print(f"âœ… æˆåŠŸåŠ è½½ {len(self.articles)} ç¯‡æ–‡ç« æ•°æ®")

            # æ ·æœ¬é‡æ£€æŸ¥
            if len(self.articles) < self.MIN_SAMPLE_SIZE:
                print(f"âš ï¸  è­¦å‘Šï¼šæ ·æœ¬é‡({len(self.articles)})å°äºå»ºè®®æœ€å°å€¼({self.MIN_SAMPLE_SIZE})ï¼Œ")
                print("   ç»Ÿè®¡ç»“æœå¯èƒ½ä¸å¤Ÿå¯é ï¼Œå»ºè®®æ”¶é›†æ›´å¤šæ•°æ®")

            return True

        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥ï¼š{e}")
            return False

    def define_viral_threshold(self, read_counts: List[int]) -> Dict[str, float]:
        """
        ä½¿ç”¨IQRæ–¹æ³•ç§‘å­¦å®šä¹‰çˆ†æ¬¾é˜ˆå€¼

        æ–¹æ³•å¯¹æ¯”ï¼š
        - IQRæ–¹æ³•ï¼šQ3 + 1.5 * IQRï¼ˆæœ€ä¸¥æ ¼ï¼Œè¯†åˆ«çœŸæ­£çš„å¼‚å¸¸å€¼ï¼‰
        - P75æ–¹æ³•ï¼š75åˆ†ä½æ•°ï¼ˆé€‚ä¸­ï¼Œè¯†åˆ«å‰25%çš„æ–‡ç« ï¼‰
        - P90æ–¹æ³•ï¼š90åˆ†ä½æ•°ï¼ˆå®½æ¾ï¼Œè¯†åˆ«å‰10%çš„æ–‡ç« ï¼‰

        Args:
            read_counts: æ‰€æœ‰æ–‡ç« çš„é˜…è¯»é‡åˆ—è¡¨

        Returns:
            åŒ…å«ä¸åŒæ–¹æ³•é˜ˆå€¼çš„å­—å…¸
        """
        if len(read_counts) < 4:
            # æ ·æœ¬å¤ªå°ï¼Œæ— æ³•è®¡ç®—IQR
            return {
                'method': 'fallback',
                'threshold': np.median(read_counts),
                'description': 'æ ·æœ¬é‡è¿‡å°ï¼Œä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºé˜ˆå€¼'
            }

        q1 = np.percentile(read_counts, 25)
        q3 = np.percentile(read_counts, 75)
        iqr = q3 - q1

        # æ–¹æ³•1ï¼šIQRå¼‚å¸¸å€¼æ£€æµ‹ï¼ˆæœ€ä¸¥æ ¼ï¼‰
        threshold_iqr = q3 + 1.5 * iqr

        # æ–¹æ³•2ï¼š75åˆ†ä½æ•°ï¼ˆæ¨èï¼‰
        threshold_p75 = q3

        # æ–¹æ³•3ï¼š90åˆ†ä½æ•°ï¼ˆå®½æ¾ï¼‰
        threshold_p90 = np.percentile(read_counts, 90)

        # æ–¹æ³•4ï¼šå‡å€¼+1æ ‡å‡†å·®ï¼ˆä¼ ç»Ÿæ–¹æ³•ï¼‰
        mean = np.mean(read_counts)
        std = np.std(read_counts)
        threshold_mean_std = mean + std

        return {
            'method': 'IQR_and_Percentile',
            'iqr_method': {
                'threshold': round(threshold_iqr, 2),
                'description': 'Q3 + 1.5*IQR (æœ€ä¸¥æ ¼)',
                'percentile': self._calculate_percentile(read_counts, threshold_iqr)
            },
            'p75_method': {
                'threshold': round(threshold_p75, 2),
                'description': '75åˆ†ä½æ•° (æ¨è)',
                'percentile': 75
            },
            'p90_method': {
                'threshold': round(threshold_p90, 2),
                'description': '90åˆ†ä½æ•° (å®½æ¾)',
                'percentile': 90
            },
            'mean_std_method': {
                'threshold': round(threshold_mean_std, 2),
                'description': 'å‡å€¼+1æ ‡å‡†å·® (ä¼ ç»Ÿ)',
                'percentile': self._calculate_percentile(read_counts, threshold_mean_std)
            },
            'recommended': 'p75_method',
            'recommended_threshold': round(threshold_p75, 2),
            'statistics': {
                'q1': round(q1, 2),
                'median': round(np.median(read_counts), 2),
                'q3': round(q3, 2),
                'iqr': round(iqr, 2),
                'mean': round(mean, 2),
                'std': round(std, 2)
            }
        }

    def _calculate_percentile(self, values: List[float], threshold: float) -> float:
        """è®¡ç®—æŸä¸ªé˜ˆå€¼å¯¹åº”çš„ç™¾åˆ†ä½æ•°"""
        values_sorted = sorted(values)
        count_below = sum(1 for v in values_sorted if v <= threshold)
        percentile = (count_below / len(values_sorted)) * 100
        return round(percentile, 1)

    def extract_keywords_tfidf(self, titles: List[str], top_n: int = 20) -> List[Tuple[str, float]]:
        """
        ä½¿ç”¨TF-IDFæ–¹æ³•æå–å…³é”®è¯ï¼ˆæ›¿ä»£å›ºå®šå…³é”®è¯åˆ—è¡¨ï¼‰

        Args:
            titles: æ–‡ç« æ ‡é¢˜åˆ—è¡¨
            top_n: è¿”å›å‰Nä¸ªå…³é”®è¯

        Returns:
            [(å…³é”®è¯, TF-IDFåˆ†æ•°), ...]
        """
        if not titles:
            return []

        # ä½¿ç”¨jiebaåˆ†è¯
        def jieba_tokenizer(text):
            # åˆ†è¯å¹¶è¿‡æ»¤åœç”¨è¯
            words = jieba.cut(text)
            # è¿‡æ»¤é•¿åº¦<=1çš„è¯
            return [w for w in words if len(w) > 1]

        try:
            # é…ç½®TF-IDFå‘é‡åŒ–å™¨
            vectorizer = TfidfVectorizer(
                tokenizer=jieba_tokenizer,
                lowercase=False,
                max_features=100,
                smooth_idf=True,  # å¹³æ»‘IDFï¼Œé¿å…é™¤é›¶
                norm='l2'         # L2å½’ä¸€åŒ–
            )

            # è®¡ç®—TF-IDFçŸ©é˜µ
            tfidf_matrix = vectorizer.fit_transform(titles)
            feature_names = vectorizer.get_feature_names_out()

            # è®¡ç®—æ¯ä¸ªè¯çš„æ€»TF-IDFå¾—åˆ†
            tfidf_scores = np.asarray(tfidf_matrix.sum(axis=0)).ravel()

            # æ’åºå¹¶è¿”å›Top N
            top_indices = tfidf_scores.argsort()[-top_n:][::-1]
            keywords = [(feature_names[i], round(tfidf_scores[i], 4))
                       for i in top_indices]

            return keywords

        except Exception as e:
            print(f"âš ï¸  TF-IDFæå–å¤±è´¥ï¼š{e}")
            # é™çº§åˆ°jieba.analyse
            return self._extract_keywords_jieba(titles, top_n)

    def _extract_keywords_jieba(self, titles: List[str], top_n: int = 20) -> List[Tuple[str, float]]:
        """ä½¿ç”¨jieba.analyseä½œä¸ºTF-IDFçš„å¤‡é€‰æ–¹æ¡ˆ"""
        all_text = ' '.join(titles)
        keywords = jieba.analyse.extract_tags(all_text, topK=top_n, withWeight=True)
        return keywords

    def classify_topics_multi_label(self, titles: List[str]) -> Dict[str, Any]:
        """
        å¤šæ ‡ç­¾è¯é¢˜åˆ†ç±»ï¼ˆä¸€ç¯‡æ–‡ç« å¯ä»¥å±äºå¤šä¸ªè¯é¢˜ï¼‰

        Args:
            titles: æ–‡ç« æ ‡é¢˜åˆ—è¡¨

        Returns:
            è¯é¢˜ç»Ÿè®¡å­—å…¸ï¼ŒåŒ…å«æƒé‡è¯„åˆ†
        """
        # å®šä¹‰è¯é¢˜æ¨¡å¼åŠå…¶æƒé‡
        topic_patterns = {
            'æµ‹è¯„ç±»': {
                'patterns': ['æµ‹è¯„', 'å®æµ‹', 'ä½“éªŒ', 'å¯¹æ¯”', 'è¯„æµ‹', 'æ·±åº¦', 'å…¨é¢'],
                'weight': 1.0
            },
            'æ•™ç¨‹ç±»': {
                'patterns': ['æ•™ç¨‹', 'æ‰‹æŠŠæ‰‹', 'è¯¦è§£', 'é…ç½®', 'å¦‚ä½•', 'æ€ä¹ˆ', 'æ­¥éª¤', 'æŒ‡å—'],
                'weight': 1.0
            },
            'å·¥å…·ç±»': {
                'patterns': ['å·¥å…·', 'æ’ä»¶', 'æ‰©å±•', 'MCP', 'API', 'é›†æˆ'],
                'weight': 1.0
            },
            'èµ„è®¯ç±»': {
                'patterns': ['æ›´æ–°', 'å‘å¸ƒ', 'æ–°åŠŸèƒ½', 'æœ€æ–°', 'å®˜æ–¹', 'æ¶ˆæ¯'],
                'weight': 0.8
            },
            'æ¡ˆä¾‹ç±»': {
                'patterns': ['æ¡ˆä¾‹', 'å®æˆ˜', 'é¡¹ç›®', 'åº”ç”¨', 'åœºæ™¯', 'ç”¨æ³•'],
                'weight': 1.0
            },
            'æŠ€æœ¯ç±»': {
                'patterns': ['æŠ€æœ¯', 'åŸç†', 'æ¶æ„', 'ç®—æ³•', 'æºç ', 'æ·±å…¥'],
                'weight': 1.2
            }
        }

        # ç»Ÿè®¡æ¯ä¸ªè¯é¢˜çš„åŒ¹é…æ¬¡æ•°å’ŒåŠ æƒå¾—åˆ†
        topic_counts = defaultdict(int)
        topic_scores = defaultdict(float)
        article_topics = []  # è®°å½•æ¯ç¯‡æ–‡ç« çš„è¯é¢˜æ ‡ç­¾

        for title in titles:
            matched_topics = []
            for topic_name, topic_info in topic_patterns.items():
                match_count = sum(1 for pattern in topic_info['patterns'] if pattern in title)
                if match_count > 0:
                    topic_counts[topic_name] += 1
                    # åŠ æƒå¾—åˆ† = åŒ¹é…æ¨¡å¼æ•° * æƒé‡
                    topic_scores[topic_name] += match_count * topic_info['weight']
                    matched_topics.append(topic_name)

            article_topics.append({
                'title': title,
                'topics': matched_topics if matched_topics else ['å…¶ä»–ç±»']
            })

        # è®¡ç®—è¯é¢˜åˆ†å¸ƒ
        total_articles = len(titles)
        topic_distribution = {
            topic: {
                'count': count,
                'percentage': round((count / total_articles) * 100, 1),
                'weighted_score': round(topic_scores[topic], 2)
            }
            for topic, count in topic_counts.items()
        }

        # æŒ‰åŠ æƒå¾—åˆ†æ’åº
        sorted_topics = sorted(
            topic_distribution.items(),
            key=lambda x: x[1]['weighted_score'],
            reverse=True
        )

        return {
            'distribution': dict(sorted_topics),
            'multi_label_enabled': True,
            'avg_topics_per_article': round(sum(len(a['topics']) for a in article_topics) / len(article_topics), 2),
            'article_topics': article_topics
        }

    def calculate_confidence_interval(self, data: List[float], confidence: float = 0.95) -> Dict[str, float]:
        """
        è®¡ç®—95%ç½®ä¿¡åŒºé—´å’Œæ ‡å‡†è¯¯

        Args:
            data: æ•°æ®åˆ—è¡¨
            confidence: ç½®ä¿¡æ°´å¹³ï¼ˆé»˜è®¤0.95ï¼‰

        Returns:
            åŒ…å«å‡å€¼ã€æ ‡å‡†è¯¯ã€ç½®ä¿¡åŒºé—´çš„å­—å…¸
        """
        if len(data) < 2:
            return {
                'mean': np.mean(data),
                'std_error': 0,
                'ci_lower': np.mean(data),
                'ci_upper': np.mean(data),
                'confidence_level': confidence
            }

        n = len(data)
        mean = np.mean(data)
        std_error = stats.sem(data)  # æ ‡å‡†è¯¯ = std / sqrt(n)

        # ä½¿ç”¨tåˆ†å¸ƒè®¡ç®—ç½®ä¿¡åŒºé—´ï¼ˆæ ·æœ¬é‡è¾ƒå°æ—¶æ›´å‡†ç¡®ï¼‰
        ci = stats.t.interval(
            confidence=confidence,
            df=n-1,  # è‡ªç”±åº¦
            loc=mean,
            scale=std_error
        )

        return {
            'mean': round(mean, 2),
            'std_error': round(std_error, 2),
            'ci_lower': round(ci[0], 2),
            'ci_upper': round(ci[1], 2),
            'confidence_level': confidence,
            'sample_size': n
        }

    def perform_ttest(self, group1: List[float], group2: List[float],
                      group1_name: str = "ç»„1", group2_name: str = "ç»„2") -> Dict[str, Any]:
        """
        æ‰§è¡Œç‹¬ç«‹æ ·æœ¬tæ£€éªŒ

        Args:
            group1: ç¬¬ä¸€ç»„æ•°æ®
            group2: ç¬¬äºŒç»„æ•°æ®
            group1_name: ç¬¬ä¸€ç»„åç§°
            group2_name: ç¬¬äºŒç»„åç§°

        Returns:
            åŒ…å«tç»Ÿè®¡é‡ã€på€¼ã€æ˜¾è‘—æ€§ç»“è®ºçš„å­—å…¸
        """
        if len(group1) < 2 or len(group2) < 2:
            return {
                'test': 't-test',
                'result': 'insufficient_data',
                'message': 'æ ·æœ¬é‡è¿‡å°ï¼Œæ— æ³•è¿›è¡Œtæ£€éªŒ'
            }

        # ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ
        t_statistic, p_value = ttest_ind(group1, group2)

        # åˆ¤æ–­æ˜¾è‘—æ€§ï¼ˆalpha = 0.05ï¼‰
        is_significant = bool(p_value < 0.05)  # è½¬æˆPythonåŸç”Ÿboolï¼Œé¿å…JSONåºåˆ—åŒ–é—®é¢˜

        # è®¡ç®—æ•ˆåº”é‡ï¼ˆCohen's dï¼‰
        mean1, mean2 = np.mean(group1), np.mean(group2)
        pooled_std = np.sqrt((np.std(group1)**2 + np.std(group2)**2) / 2)
        cohens_d = (mean1 - mean2) / pooled_std if pooled_std > 0 else 0

        return {
            'test': 't-test',
            'group1': {
                'name': group1_name,
                'mean': round(mean1, 2),
                'std': round(np.std(group1), 2),
                'n': len(group1)
            },
            'group2': {
                'name': group2_name,
                'mean': round(mean2, 2),
                'std': round(np.std(group2), 2),
                'n': len(group2)
            },
            't_statistic': round(t_statistic, 4),
            'p_value': round(p_value, 4),
            'is_significant': is_significant,
            'significance_level': 0.05,
            'cohens_d': round(cohens_d, 4),
            'effect_size': self._interpret_cohens_d(cohens_d),
            'conclusion': self._interpret_ttest(is_significant, mean1, mean2, group1_name, group2_name)
        }

    def _interpret_cohens_d(self, d: float) -> str:
        """è§£é‡ŠCohen's dæ•ˆåº”é‡"""
        abs_d = abs(d)
        if abs_d < 0.2:
            return "å¾®å°æ•ˆåº”"
        elif abs_d < 0.5:
            return "å°æ•ˆåº”"
        elif abs_d < 0.8:
            return "ä¸­ç­‰æ•ˆåº”"
        else:
            return "å¤§æ•ˆåº”"

    def _interpret_ttest(self, is_significant: bool, mean1: float, mean2: float,
                        name1: str, name2: str) -> str:
        """è§£é‡Štæ£€éªŒç»“æœ"""
        if not is_significant:
            return f"âŒ æ— æ˜¾è‘—å·®å¼‚ï¼š{name1}å’Œ{name2}çš„å‡å€¼å·®å¼‚ä¸æ˜¾è‘— (p â‰¥ 0.05)"

        if mean1 > mean2:
            return f"âœ… æ˜¾è‘—å·®å¼‚ï¼š{name1}æ˜¾è‘—é«˜äº{name2} (p < 0.05)"
        else:
            return f"âœ… æ˜¾è‘—å·®å¼‚ï¼š{name2}æ˜¾è‘—é«˜äº{name1} (p < 0.05)"

    def analyze_time_series(self, articles: List[Dict]) -> Dict[str, Any]:
        """
        æ”¹è¿›çš„æ—¶é—´åºåˆ—åˆ†æ

        - å…·ä½“æ—¶é—´æ®µï¼ˆæ¯2å°æ—¶ä¸€ä¸ªæ—¶æ®µï¼‰
        - æ¯æ—¥è¶‹åŠ¿åˆ†æ
        - æœ€ä½³å‘å¸ƒæ—¶é—´æ¨è
        """
        time_slot_data = defaultdict(lambda: {
            'count': 0,
            'read_counts': [],
            'like_counts': []
        })

        weekday_data = defaultdict(lambda: {
            'count': 0,
            'read_counts': []
        })

        for article in articles:
            pub_time_str = article.get('publish_time', '')
            if not pub_time_str:
                continue

            try:
                # è§£ææ—¶é—´ - å°è¯•å¤šç§æ ¼å¼
                pub_time = None
                # å°è¯•ISOæ ¼å¼
                try:
                    pub_time = datetime.fromisoformat(pub_time_str.replace('Z', '+00:00'))
                except:
                    # å°è¯•ä»ä¸­æ–‡æ ¼å¼æå– "ä»Šå¤© 18:35" æˆ– "æ˜¨å¤© 20:00"
                    time_match = re.search(r'(\d{1,2}):(\d{2})', pub_time_str)
                    if time_match:
                        hour = int(time_match.group(1))
                        minute = int(time_match.group(2))
                        # ä½¿ç”¨å½“å‰æ—¥æœŸ
                        pub_time = datetime.now().replace(hour=hour, minute=minute, second=0, microsecond=0)

                if not pub_time:
                    continue

                hour = pub_time.hour
                weekday = pub_time.strftime('%A')  # Monday, Tuesday, ...

                # 2å°æ—¶æ—¶æ®µåˆ†ç±»
                time_slot = self._get_time_slot(hour)

                # è®°å½•æ•°æ®
                read_count = article.get('read_count', 0)
                like_count = article.get('like_count', 0)

                time_slot_data[time_slot]['count'] += 1
                time_slot_data[time_slot]['read_counts'].append(read_count)
                time_slot_data[time_slot]['like_counts'].append(like_count)

                weekday_data[weekday]['count'] += 1
                weekday_data[weekday]['read_counts'].append(read_count)

            except Exception as e:
                continue

        # è®¡ç®—æ¯ä¸ªæ—¶æ®µçš„ç»Ÿè®¡é‡
        time_analysis = {}
        for slot, data in time_slot_data.items():
            if data['count'] > 0:
                avg_read = np.mean(data['read_counts'])
                avg_like = np.mean(data['like_counts'])
                time_analysis[slot] = {
                    'count': data['count'],
                    'avg_read': round(avg_read, 2),
                    'avg_like': round(avg_like, 2),
                    'total_read': sum(data['read_counts']),
                    'ci_read': self.calculate_confidence_interval(data['read_counts'])
                }

        # æ‰¾å‡ºæœ€ä½³æ—¶æ®µ
        best_slot = max(time_analysis.items(),
                       key=lambda x: x[1]['avg_read']) if time_analysis else (None, None)

        # æ¯å‘¨è¶‹åŠ¿
        weekday_analysis = {}
        for day, data in weekday_data.items():
            if data['count'] > 0:
                weekday_analysis[day] = {
                    'count': data['count'],
                    'avg_read': round(np.mean(data['read_counts']), 2),
                    'ci_read': self.calculate_confidence_interval(data['read_counts'])
                }

        return {
            'time_slot_analysis': dict(sorted(time_analysis.items())),
            'weekday_analysis': weekday_analysis,
            'best_time_slot': {
                'slot': best_slot[0],
                'stats': best_slot[1]
            } if best_slot[0] else None,
            'recommendation': self._generate_time_recommendation(time_analysis)
        }

    def _get_time_slot(self, hour: int) -> str:
        """å°†å°æ—¶è½¬æ¢ä¸º2å°æ—¶æ—¶æ®µ"""
        slots = {
            (0, 2): "00:00-02:00 (æ·±å¤œ)",
            (2, 4): "02:00-04:00 (æ·±å¤œ)",
            (4, 6): "04:00-06:00 (å‡Œæ™¨)",
            (6, 8): "06:00-08:00 (æ—©æ™¨)",
            (8, 10): "08:00-10:00 (ä¸Šåˆ)",
            (10, 12): "10:00-12:00 (ä¸Šåˆ)",
            (12, 14): "12:00-14:00 (ä¸­åˆ)",
            (14, 16): "14:00-16:00 (ä¸‹åˆ)",
            (16, 18): "16:00-18:00 (ä¸‹åˆ)",
            (18, 20): "18:00-20:00 (å‚æ™š)",
            (20, 22): "20:00-22:00 (æ™šä¸Š)",
            (22, 24): "22:00-24:00 (æ·±å¤œ)"
        }

        for (start, end), slot_name in slots.items():
            if start <= hour < end:
                return slot_name
        return "æœªçŸ¥æ—¶æ®µ"

    def _generate_time_recommendation(self, time_analysis: Dict) -> str:
        """ç”Ÿæˆå‘å¸ƒæ—¶é—´å»ºè®®"""
        if not time_analysis:
            return "æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç»™å‡ºå»ºè®®"

        # æ‰¾å‡ºè¡¨ç°æœ€å¥½çš„3ä¸ªæ—¶æ®µ
        top_slots = sorted(time_analysis.items(),
                          key=lambda x: x[1]['avg_read'],
                          reverse=True)[:3]

        recommendations = []
        for i, (slot, stats) in enumerate(top_slots, 1):
            recommendations.append(
                f"{i}. {slot}ï¼šå¹³å‡é˜…è¯»{stats['avg_read']:.0f} (95% CI: {stats['ci_read']['ci_lower']:.0f}-{stats['ci_read']['ci_upper']:.0f})"
            )

        return "\n".join(["ğŸ“… æœ€ä½³å‘å¸ƒæ—¶æ®µï¼š"] + recommendations)

    def run_analysis(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„æ•°æ®åˆ†ææµç¨‹

        Returns:
            å®Œæ•´çš„åˆ†æç»“æœå­—å…¸
        """
        if not self.articles:
            print("âŒ æ²¡æœ‰æ•°æ®å¯åˆ†æ")
            return {}

        print("\n" + "="*60)
        print("ğŸ“Š å¼€å§‹æ•°æ®åˆ†æ (ç§‘å­¦ç»Ÿè®¡ç‰ˆ V7.0)")
        print("="*60 + "\n")

        # 1. åŸºç¡€ç»Ÿè®¡
        print("ğŸ“ˆ æ­¥éª¤1/7ï¼šè®¡ç®—åŸºç¡€ç»Ÿè®¡é‡...")
        read_counts = [a.get('read_count', 0) for a in self.articles]
        like_counts = [a.get('like_count', 0) for a in self.articles]

        # 2. ç§‘å­¦å®šä¹‰çˆ†æ¬¾é˜ˆå€¼
        print("ğŸ¯ æ­¥éª¤2/7ï¼šä½¿ç”¨IQRæ–¹æ³•å®šä¹‰çˆ†æ¬¾é˜ˆå€¼...")
        viral_threshold_info = self.define_viral_threshold(read_counts)
        recommended_threshold = viral_threshold_info['recommended_threshold']

        # åˆ’åˆ†çˆ†æ¬¾å’Œæ™®é€šæ–‡ç« 
        viral_articles = [a for a in self.articles if a.get('read_count', 0) >= recommended_threshold]
        normal_articles = [a for a in self.articles if a.get('read_count', 0) < recommended_threshold]

        print(f"   æ¨èé˜ˆå€¼ï¼š{recommended_threshold:.0f} (P75æ–¹æ³•)")
        print(f"   çˆ†æ¬¾æ–‡ç« ï¼š{len(viral_articles)} ç¯‡ ({len(viral_articles)/len(self.articles)*100:.1f}%)")
        print(f"   æ™®é€šæ–‡ç« ï¼š{len(normal_articles)} ç¯‡")

        # 3. TF-IDFå…³é”®è¯æå–
        print("ğŸ” æ­¥éª¤3/7ï¼šä½¿ç”¨TF-IDFæå–å…³é”®è¯...")
        all_titles = [a.get('title', '') for a in self.articles]
        viral_titles = [a.get('title', '') for a in viral_articles]

        keywords_all = self.extract_keywords_tfidf(all_titles, top_n=20)
        keywords_viral = self.extract_keywords_tfidf(viral_titles, top_n=15) if viral_titles else []

        print(f"   æå–åˆ° {len(keywords_all)} ä¸ªé«˜é¢‘å…³é”®è¯")

        # 4. æ ‡é¢˜é•¿åº¦åˆ†æ + tæ£€éªŒ
        print("ğŸ“ æ­¥éª¤4/7ï¼šåˆ†ææ ‡é¢˜é•¿åº¦å¹¶æ‰§è¡Œç»Ÿè®¡æ£€éªŒ...")
        viral_title_lengths = [len(a.get('title', '')) for a in viral_articles]
        normal_title_lengths = [len(a.get('title', '')) for a in normal_articles]

        title_length_ttest = self.perform_ttest(
            viral_title_lengths,
            normal_title_lengths,
            "çˆ†æ¬¾æ–‡ç« æ ‡é¢˜é•¿åº¦",
            "æ™®é€šæ–‡ç« æ ‡é¢˜é•¿åº¦"
        )

        title_length_ci = self.calculate_confidence_interval(viral_title_lengths) if viral_title_lengths else {}

        # 5. å¤šæ ‡ç­¾è¯é¢˜åˆ†ç±»
        print("ğŸ·ï¸  æ­¥éª¤5/7ï¼šæ‰§è¡Œå¤šæ ‡ç­¾è¯é¢˜åˆ†ç±»...")
        topic_analysis = self.classify_topics_multi_label(all_titles)
        viral_topic_analysis = self.classify_topics_multi_label(viral_titles) if viral_titles else {}

        # 6. æ—¶é—´åºåˆ—åˆ†æ
        print("â° æ­¥éª¤6/7ï¼šåˆ†æå‘å¸ƒæ—¶é—´è¶‹åŠ¿...")
        time_analysis = self.analyze_time_series(self.articles)

        # 7. é˜…è¯»é‡ä¸ç‚¹èµæ•°ç›¸å…³æ€§
        print("ğŸ’¡ æ­¥éª¤7/7ï¼šè®¡ç®—æŒ‡æ ‡ç›¸å…³æ€§...")
        read_like_correlation = np.corrcoef(read_counts, like_counts)[0, 1] if len(read_counts) > 1 else 0

        # æ±‡æ€»ç»“æœ
        self.analysis_results = {
            'metadata': {
                'analysis_date': datetime.now().isoformat(),
                'total_articles': len(self.articles),
                'analysis_version': 'V7.0_Scientific',
                'sample_size_warning': len(self.articles) < self.MIN_SAMPLE_SIZE
            },
            'viral_threshold': viral_threshold_info,
            'viral_stats': {
                'viral_count': len(viral_articles),
                'viral_rate': round(len(viral_articles) / len(self.articles) * 100, 1),
                'threshold_used': recommended_threshold,
                'method': 'P75 (recommended)'
            },
            'keywords': {
                'all_articles': keywords_all[:10],
                'viral_articles': keywords_viral[:10],
                'extraction_method': 'TF-IDF with jieba'
            },
            'title_length': {
                'ttest_results': title_length_ttest,
                'viral_length_ci': title_length_ci,
                'recommendation': self._generate_title_length_recommendation(title_length_ci)
            },
            'topics': {
                'all_articles': topic_analysis,
                'viral_articles': viral_topic_analysis,
                'classification_method': 'multi_label_weighted'
            },
            'time_analysis': time_analysis,
            'correlations': {
                'read_like_correlation': round(read_like_correlation, 4),
                'interpretation': self._interpret_correlation(read_like_correlation)
            },
            'recommendations': self._generate_recommendations()
        }

        print("\nâœ… åˆ†æå®Œæˆï¼")
        return self.analysis_results

    def _generate_title_length_recommendation(self, ci_info: Dict) -> str:
        """ç”Ÿæˆæ ‡é¢˜é•¿åº¦å»ºè®®"""
        if not ci_info:
            return "æ•°æ®ä¸è¶³"

        mean = ci_info['mean']
        ci_lower = ci_info['ci_lower']
        ci_upper = ci_info['ci_upper']

        return f"å»ºè®®æ ‡é¢˜é•¿åº¦ï¼š{mean:.0f}å­— (95% CI: {ci_lower:.0f}-{ci_upper:.0f}å­—)"

    def _interpret_correlation(self, r: float) -> str:
        """è§£é‡Šç›¸å…³ç³»æ•°"""
        abs_r = abs(r)
        if abs_r < 0.3:
            strength = "å¼±ç›¸å…³"
        elif abs_r < 0.7:
            strength = "ä¸­ç­‰ç›¸å…³"
        else:
            strength = "å¼ºç›¸å…³"

        direction = "æ­£ç›¸å…³" if r > 0 else "è´Ÿç›¸å…³"
        return f"{direction}ï¼Œ{strength} (r={r:.3f})"

    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ•°æ®é©±åŠ¨çš„å»ºè®®"""
        recommendations = []

        # åŸºäºå…³é”®è¯
        if self.analysis_results.get('keywords', {}).get('viral_articles'):
            top_keywords = self.analysis_results['keywords']['viral_articles'][:5]
            kw_list = ', '.join([kw[0] for kw in top_keywords])
            recommendations.append(f"ğŸ’¡ é«˜é¢‘å…³é”®è¯å»ºè®®ï¼šä¼˜å…ˆä½¿ç”¨ã€Œ{kw_list}ã€ç­‰è¯æ±‡")

        # åŸºäºæ ‡é¢˜é•¿åº¦
        if self.analysis_results.get('title_length', {}).get('viral_length_ci'):
            rec = self.analysis_results['title_length']['recommendation']
            recommendations.append(f"ğŸ“ æ ‡é¢˜é•¿åº¦å»ºè®®ï¼š{rec}")

        # åŸºäºè¯é¢˜
        viral_topics = self.analysis_results.get('topics', {}).get('viral_articles', {}).get('distribution', {})
        if viral_topics:
            best_topic = max(viral_topics.items(), key=lambda x: x[1]['weighted_score'])
            recommendations.append(f"ğŸ·ï¸  è¯é¢˜å»ºè®®ï¼šã€Œ{best_topic[0]}ã€è¡¨ç°æœ€ä½³ (åŠ æƒå¾—åˆ†: {best_topic[1]['weighted_score']})")

        # åŸºäºæ—¶é—´
        if self.analysis_results.get('time_analysis', {}).get('recommendation'):
            recommendations.append(self.analysis_results['time_analysis']['recommendation'])

        return recommendations

    def print_report(self):
        """æ‰“å°ç¾åŒ–çš„åˆ†ææŠ¥å‘Š"""
        if not self.analysis_results:
            print("âŒ æ²¡æœ‰åˆ†æç»“æœ")
            return

        print("\n" + "="*60)
        print("ğŸ“Š å¾®ä¿¡å…¬ä¼—å·æ•°æ®åˆ†ææŠ¥å‘Š (ç§‘å­¦ç»Ÿè®¡ç‰ˆ V7.0)")
        print("="*60 + "\n")

        # å…ƒæ•°æ®
        meta = self.analysis_results['metadata']
        print(f"ğŸ“… åˆ†ææ—¥æœŸï¼š{meta['analysis_date'][:19]}")
        print(f"ğŸ“š æ ·æœ¬é‡ï¼š{meta['total_articles']} ç¯‡æ–‡ç« ")
        if meta['sample_size_warning']:
            print(f"âš ï¸  æ ·æœ¬é‡è­¦å‘Šï¼šæ ·æœ¬é‡å°äº{self.MIN_SAMPLE_SIZE}ï¼Œç»“æœå¯èƒ½ä¸å¤Ÿå¯é ")
        print()

        # çˆ†æ¬¾é˜ˆå€¼
        print("ğŸ¯ çˆ†æ¬¾é˜ˆå€¼å®šä¹‰ï¼ˆIQRæ–¹æ³•ï¼‰")
        print("-" * 60)
        threshold_info = self.analysis_results['viral_threshold']
        stats = threshold_info['statistics']
        print(f"   åŸºç¡€ç»Ÿè®¡ï¼šQ1={stats['q1']}, ä¸­ä½æ•°={stats['median']}, Q3={stats['q3']}")
        print(f"   IQR = {stats['iqr']}, å‡å€¼={stats['mean']}, æ ‡å‡†å·®={stats['std']}")
        print()
        print("   å„æ–¹æ³•é˜ˆå€¼å¯¹æ¯”ï¼š")
        for method_key, method_name in [('p75_method', 'P75'), ('p90_method', 'P90'),
                                         ('iqr_method', 'IQR'), ('mean_std_method', 'Mean+STD')]:
            method_data = threshold_info[method_key]
            print(f"   - {method_name}ï¼š{method_data['threshold']:.0f} ({method_data['description']})")

        print(f"\n   âœ… æ¨èæ–¹æ³•ï¼šP75 (é˜ˆå€¼={threshold_info['recommended_threshold']:.0f})")
        viral_stats = self.analysis_results['viral_stats']
        print(f"   çˆ†æ¬¾æ–‡ç« ï¼š{viral_stats['viral_count']} ç¯‡ ({viral_stats['viral_rate']}%)")
        print()

        # å…³é”®è¯ï¼ˆTF-IDFï¼‰
        print("ğŸ” é«˜é¢‘å…³é”®è¯ï¼ˆTF-IDFæå–ï¼‰")
        print("-" * 60)
        keywords_all = self.analysis_results['keywords']['all_articles']
        print("   æ‰€æœ‰æ–‡ç« TOP 10ï¼š")
        for i, (kw, score) in enumerate(keywords_all[:10], 1):
            print(f"   {i}. {kw} (TF-IDF: {score:.4f})")

        keywords_viral = self.analysis_results['keywords']['viral_articles']
        if keywords_viral:
            print("\n   çˆ†æ¬¾æ–‡ç« TOP 10ï¼š")
            for i, (kw, score) in enumerate(keywords_viral[:10], 1):
                print(f"   {i}. {kw} (TF-IDF: {score:.4f})")
        print()

        # æ ‡é¢˜é•¿åº¦ï¼ˆå«tæ£€éªŒï¼‰
        print("ğŸ“ æ ‡é¢˜é•¿åº¦åˆ†æï¼ˆå«ç»Ÿè®¡æ£€éªŒï¼‰")
        print("-" * 60)
        title_analysis = self.analysis_results['title_length']
        ttest = title_analysis['ttest_results']

        if ttest['test'] == 't-test':
            print(f"   çˆ†æ¬¾æ–‡ç« ï¼šå‡å€¼={ttest['group1']['mean']}å­—, æ ‡å‡†å·®={ttest['group1']['std']}, n={ttest['group1']['n']}")
            print(f"   æ™®é€šæ–‡ç« ï¼šå‡å€¼={ttest['group2']['mean']}å­—, æ ‡å‡†å·®={ttest['group2']['std']}, n={ttest['group2']['n']}")
            print(f"\n   tæ£€éªŒç»“æœï¼š")
            print(f"   - tç»Ÿè®¡é‡ï¼š{ttest['t_statistic']}")
            print(f"   - på€¼ï¼š{ttest['p_value']}")
            print(f"   - æ˜¾è‘—æ€§ï¼š{'æ˜¯' if ttest['is_significant'] else 'å¦'} (Î±=0.05)")
            print(f"   - æ•ˆåº”é‡ï¼š{ttest['effect_size']} (Cohen's d={ttest['cohens_d']})")
            print(f"   - ç»“è®ºï¼š{ttest['conclusion']}")

        if title_analysis.get('viral_length_ci'):
            ci = title_analysis['viral_length_ci']
            print(f"\n   çˆ†æ¬¾æ ‡é¢˜é•¿åº¦95%ç½®ä¿¡åŒºé—´ï¼š[{ci['ci_lower']}, {ci['ci_upper']}]å­—")
            print(f"   æ ‡å‡†è¯¯ï¼š{ci['std_error']}")

        print(f"\n   ğŸ“Œ {title_analysis['recommendation']}")
        print()

        # è¯é¢˜åˆ†ç±»ï¼ˆå¤šæ ‡ç­¾ï¼‰
        print("ğŸ·ï¸  è¯é¢˜åˆ†ç±»ï¼ˆå¤šæ ‡ç­¾æƒé‡è¯„åˆ†ï¼‰")
        print("-" * 60)
        topic_info = self.analysis_results['topics']['all_articles']
        print(f"   å¤šæ ‡ç­¾æ¨¡å¼ï¼šå¯ç”¨")
        print(f"   å¹³å‡æ ‡ç­¾æ•°/æ–‡ç« ï¼š{topic_info['avg_topics_per_article']}")
        print("\n   è¯é¢˜åˆ†å¸ƒï¼ˆæŒ‰åŠ æƒå¾—åˆ†æ’åºï¼‰ï¼š")
        for topic, stats in topic_info['distribution'].items():
            print(f"   - {topic}ï¼š{stats['count']}ç¯‡ ({stats['percentage']}%), åŠ æƒå¾—åˆ†={stats['weighted_score']}")

        viral_topics = self.analysis_results['topics'].get('viral_articles', {}).get('distribution', {})
        if viral_topics:
            print("\n   çˆ†æ¬¾æ–‡ç« è¯é¢˜åˆ†å¸ƒï¼š")
            for topic, stats in list(viral_topics.items())[:5]:
                print(f"   - {topic}ï¼š{stats['count']}ç¯‡, åŠ æƒå¾—åˆ†={stats['weighted_score']}")
        print()

        # æ—¶é—´åˆ†æ
        print("â° å‘å¸ƒæ—¶é—´åˆ†æï¼ˆ2å°æ—¶æ—¶æ®µï¼‰")
        print("-" * 60)
        time_info = self.analysis_results['time_analysis']
        best_slot = time_info.get('best_time_slot')
        if best_slot and best_slot['slot']:
            stats = best_slot['stats']
            print(f"   ğŸ† æœ€ä½³æ—¶æ®µï¼š{best_slot['slot']}")
            print(f"      - å¹³å‡é˜…è¯»ï¼š{stats['avg_read']:.0f}")
            print(f"      - 95% CIï¼š[{stats['ci_read']['ci_lower']:.0f}, {stats['ci_read']['ci_upper']:.0f}]")
            print(f"      - æ–‡ç« æ•°ï¼š{stats['count']}")

        print(f"\n{time_info['recommendation']}")
        print()

        # ç›¸å…³æ€§åˆ†æ
        print("ğŸ’¡ æŒ‡æ ‡ç›¸å…³æ€§åˆ†æ")
        print("-" * 60)
        corr_info = self.analysis_results['correlations']
        print(f"   é˜…è¯»é‡ vs ç‚¹èµæ•°ï¼š{corr_info['interpretation']}")
        print()

        # ç»¼åˆå»ºè®®
        print("ğŸ¯ æ•°æ®é©±åŠ¨çš„å†…å®¹ç­–ç•¥å»ºè®®")
        print("-" * 60)
        for rec in self.analysis_results['recommendations']:
            print(f"   {rec}")

        print("\n" + "="*60)
        print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        print("="*60 + "\n")

    def save_report(self, output_file: str = None):
        """
        ä¿å­˜åˆ†ææŠ¥å‘Šä¸ºJSONæ–‡ä»¶

        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•çš„analysis_report.json
        """
        if not self.analysis_results:
            print("âŒ æ²¡æœ‰åˆ†æç»“æœå¯ä¿å­˜")
            return

        if output_file is None:
            output_file = Path.cwd() / "analysis_report.json"
        else:
            output_file = Path(output_file)

        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.analysis_results, f, ensure_ascii=False, indent=2)

            print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°ï¼š{output_file}")

        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥ï¼š{e}")


def main():
    """ä¸»å‡½æ•°"""
    # å¯¼å…¥PROJECT_ROOTï¼ˆä¿®å¤ï¼šä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
    import os
    from pathlib import Path

    PROJECT_ROOT = Path(os.getenv('CLAUDE_PROJECT_DIR', Path(__file__).parent.parent.parent.parent.parent))
    default_data_file = PROJECT_ROOT / "data" / "wechat_articles.json"

    # å¦‚æœå‘½ä»¤è¡Œæä¾›äº†å‚æ•°ï¼Œä½¿ç”¨å‚æ•°
    data_file = sys.argv[1] if len(sys.argv) > 1 else str(default_data_file)

    print("="*60)
    print("ğŸ“Š å¾®ä¿¡å…¬ä¼—å·æ•°æ®åˆ†æå·¥å…· V7.0 - ç§‘å­¦ç»Ÿè®¡ç‰ˆ")
    print("="*60)
    print("\næ”¹è¿›å†…å®¹ï¼š")
    print("âœ… IQRæ–¹æ³•å®šä¹‰çˆ†æ¬¾é˜ˆå€¼ï¼ˆæ›¿ä»£ç¡¬ç¼–ç 1000ï¼‰")
    print("âœ… TF-IDFå…³é”®è¯æå–ï¼ˆæ›¿ä»£å›ºå®šå…³é”®è¯åˆ—è¡¨ï¼‰")
    print("âœ… ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒï¼ˆt-testã€p-valueã€Cohen's dï¼‰")
    print("âœ… 95%ç½®ä¿¡åŒºé—´å’Œæ ‡å‡†è¯¯")
    print("âœ… å¤šæ ‡ç­¾è¯é¢˜åˆ†ç±»ï¼ˆåŠ æƒè¯„åˆ†ï¼‰")
    print("âœ… æ”¹è¿›çš„æ—¶é—´åºåˆ—åˆ†æï¼ˆ2å°æ—¶æ—¶æ®µï¼‰")
    print()

    # åˆ›å»ºåˆ†æå™¨
    analyzer = WeChatDataAnalyzer(data_file)

    # åŠ è½½æ•°æ®
    if not analyzer.load_data():
        sys.exit(1)

    # æ‰§è¡Œåˆ†æ
    analyzer.run_analysis()

    # æ‰“å°æŠ¥å‘Š
    analyzer.print_report()

    # ä¿å­˜æŠ¥å‘Š
    output_file = Path.cwd() / "analysis_report.json"
    analyzer.save_report(str(output_file))

    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®ï¼š")
    print("   1. æ ¹æ®TF-IDFé«˜é¢‘å…³é”®è¯é€‰é¢˜")
    print("   2. æ ‡é¢˜é•¿åº¦æ§åˆ¶åœ¨æ¨èçš„95%ç½®ä¿¡åŒºé—´å†…")
    print("   3. ä¼˜å…ˆåˆ›ä½œé«˜åŠ æƒå¾—åˆ†çš„è¯é¢˜ç±»å‹")
    print("   4. åœ¨æœ€ä½³æ—¶æ®µå‘å¸ƒæ–‡ç« ")
    print("   5. å®šæœŸæ”¶é›†æ•°æ®æ›´æ–°åˆ†æç»“æœ\n")


if __name__ == "__main__":
    main()
