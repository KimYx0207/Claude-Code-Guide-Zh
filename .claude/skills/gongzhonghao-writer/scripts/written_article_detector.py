#!/usr/bin/env python3
"""
å·²å†™æ–‡ç« æ£€æµ‹å™¨
æ£€æµ‹çƒ­ç‚¹æ˜¯å¦å·²è¢«å†™è¿‡ï¼Œé¿å…é‡å¤åˆ›ä½œ
"""

import os
import re
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta
from difflib import SequenceMatcher


class WrittenArticleDetector:
    """å·²å†™æ–‡ç« æ£€æµ‹å™¨"""

    def __init__(self, articles_dir: str = None):
        """
        åˆå§‹åŒ–æ£€æµ‹å™¨

        Args:
            articles_dir: æ–‡ç« ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é¡¹ç›®articlesç›®å½•
        """
        if articles_dir is None:
            # é»˜è®¤è·¯å¾„
            self.articles_dir = Path(__file__).parent.parent.parent.parent.parent / "articles"
        else:
            self.articles_dir = Path(articles_dir)

        self.articles_cache: List[Dict] = []
        self._load_articles()

    def _load_articles(self) -> None:
        """åŠ è½½æ‰€æœ‰å·²å†™æ–‡ç« """
        self.articles_cache = []

        if not self.articles_dir.exists():
            return

        for file_path in self.articles_dir.glob("*.md"):
            article = self._parse_article(file_path)
            if article:
                self.articles_cache.append(article)

        # æŒ‰æ—¥æœŸæ’åº
        self.articles_cache.sort(key=lambda x: x.get("date", ""), reverse=True)

    def _parse_article(self, file_path: Path) -> Optional[Dict]:
        """
        è§£ææ–‡ç« æ–‡ä»¶

        Args:
            file_path: æ–‡ç« æ–‡ä»¶è·¯å¾„

        Returns:
            æ–‡ç« ä¿¡æ¯å­—å…¸
        """
        try:
            filename = file_path.name

            # è§£ææ–‡ä»¶åæ ¼å¼: 2025-11-24_æ ‡é¢˜_è€é‡‘é£æ ¼.md
            match = re.match(r"(\d{4}-\d{2}-\d{2})_(.+?)(?:_è€é‡‘é£æ ¼)?\.md", filename)
            if not match:
                return None

            date_str = match.group(1)
            title = match.group(2)

            # è¯»å–æ–‡ç« å†…å®¹æå–å…³é”®è¯
            content = file_path.read_text(encoding="utf-8")
            keywords = self._extract_keywords(title, content)

            return {
                "path": str(file_path),
                "filename": filename,
                "date": date_str,
                "title": title,
                "keywords": keywords,
                "content_preview": content[:500] if content else ""
            }
        except Exception as e:
            return None

    def _extract_keywords(self, title: str, content: str) -> List[str]:
        """
        ä»æ ‡é¢˜å’Œå†…å®¹ä¸­æå–å…³é”®è¯

        Args:
            title: æ–‡ç« æ ‡é¢˜
            content: æ–‡ç« å†…å®¹

        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        keywords = set()

        # å¸¸è§AIç›¸å…³å…³é”®è¯
        ai_keywords = [
            "Claude", "GPT", "ChatGPT", "OpenAI", "Anthropic", "Google", "Gemini",
            "MCP", "Agent", "LLM", "å¤§æ¨¡å‹", "AI", "äººå·¥æ™ºèƒ½",
            "Cursor", "Copilot", "Code", "ç¼–ç¨‹", "å¼€å‘",
            "Midjourney", "Stable Diffusion", "DALL-E", "AIç»˜ç”»",
            "Suno", "AIéŸ³ä¹", "è¯­éŸ³",
            "Hook", "Skill", "Command", "å‘½ä»¤", "æŠ€å·§",
            "æ•ˆç‡", "å·¥å…·", "é…ç½®", "æ•™ç¨‹"
        ]

        text = f"{title} {content[:2000]}".lower()

        for kw in ai_keywords:
            if kw.lower() in text:
                keywords.add(kw)

        # ä»æ ‡é¢˜ä¸­æå–ç‰¹æ®Šè¯
        title_words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', title)
        for word in title_words:
            if len(word) >= 2:
                keywords.add(word)

        return list(keywords)

    def check_hotspot(self, hotspot_title: str, hotspot_keywords: List[str] = None) -> Dict:
        """
        æ£€æŸ¥çƒ­ç‚¹æ˜¯å¦å·²è¢«å†™è¿‡

        Args:
            hotspot_title: çƒ­ç‚¹æ ‡é¢˜
            hotspot_keywords: çƒ­ç‚¹å…³é”®è¯åˆ—è¡¨

        Returns:
            æ£€æµ‹ç»“æœå­—å…¸
        """
        if hotspot_keywords is None:
            hotspot_keywords = []

        # åˆå¹¶æ ‡é¢˜ä¸­çš„å…³é”®è¯
        title_keywords = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', hotspot_title)
        all_keywords = set(hotspot_keywords + [k for k in title_keywords if len(k) >= 2])

        result = {
            "hotspot_title": hotspot_title,
            "is_written": False,
            "similar_articles": [],
            "recommendation": "å¯ä»¥å†™",
            "confidence": 0.0
        }

        for article in self.articles_cache:
            similarity = self._calculate_similarity(
                hotspot_title,
                all_keywords,
                article
            )

            if similarity > 0.3:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                result["similar_articles"].append({
                    "title": article["title"],
                    "date": article["date"],
                    "similarity": round(similarity, 2),
                    "path": article["path"]
                })

        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        result["similar_articles"].sort(key=lambda x: x["similarity"], reverse=True)

        # åˆ¤æ–­æ˜¯å¦å·²å†™è¿‡
        if result["similar_articles"]:
            top_similarity = result["similar_articles"][0]["similarity"]
            result["confidence"] = top_similarity

            if top_similarity >= 0.7:
                result["is_written"] = True
                result["recommendation"] = f"âš ï¸ é«˜åº¦ç›¸ä¼¼ï¼å·²æœ‰æ–‡ç« ã€Š{result['similar_articles'][0]['title']}ã€‹({result['similar_articles'][0]['date']})"
            elif top_similarity >= 0.5:
                result["is_written"] = False
                result["recommendation"] = f"âš¡ æœ‰ç›¸ä¼¼æ–‡ç« ï¼Œå»ºè®®æ¢è§’åº¦ï¼šã€Š{result['similar_articles'][0]['title']}ã€‹"
            else:
                result["recommendation"] = "âœ… å¯ä»¥å†™ï¼Œæ— é«˜åº¦ç›¸ä¼¼æ–‡ç« "

        return result

    def _calculate_similarity(self, hotspot_title: str, hotspot_keywords: set, article: Dict) -> float:
        """
        è®¡ç®—çƒ­ç‚¹ä¸å·²æœ‰æ–‡ç« çš„ç›¸ä¼¼åº¦

        Args:
            hotspot_title: çƒ­ç‚¹æ ‡é¢˜
            hotspot_keywords: çƒ­ç‚¹å…³é”®è¯é›†åˆ
            article: å·²æœ‰æ–‡ç« ä¿¡æ¯

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        scores = []

        # 1. æ ‡é¢˜ç›¸ä¼¼åº¦ (æƒé‡ 0.4)
        title_sim = SequenceMatcher(None, hotspot_title.lower(), article["title"].lower()).ratio()
        scores.append(("title", title_sim, 0.4))

        # 2. å…³é”®è¯é‡å åº¦ (æƒé‡ 0.4)
        article_keywords = set(k.lower() for k in article.get("keywords", []))
        hotspot_kw_lower = set(k.lower() for k in hotspot_keywords)

        if hotspot_kw_lower and article_keywords:
            overlap = len(hotspot_kw_lower & article_keywords)
            total = len(hotspot_kw_lower | article_keywords)
            keyword_sim = overlap / total if total > 0 else 0
        else:
            keyword_sim = 0
        scores.append(("keywords", keyword_sim, 0.4))

        # 3. ç‰¹å®šäº§å“/å·¥å…·åç§°å®Œå…¨åŒ¹é… (æƒé‡ 0.2)
        product_names = ["Claude", "GPT", "Gemini", "Cursor", "MCP", "Midjourney", "Suno"]
        product_match = 0
        for product in product_names:
            if product.lower() in hotspot_title.lower() and product.lower() in article["title"].lower():
                product_match = 1
                break
        scores.append(("product", product_match, 0.2))

        # è®¡ç®—åŠ æƒæ€»åˆ†
        total_score = sum(score * weight for _, score, weight in scores)

        return total_score

    def batch_check(self, hotspots: List[Dict]) -> List[Dict]:
        """
        æ‰¹é‡æ£€æŸ¥å¤šä¸ªçƒ­ç‚¹

        Args:
            hotspots: çƒ­ç‚¹åˆ—è¡¨ï¼Œæ¯ä¸ªçƒ­ç‚¹åŒ…å«titleå’Œkeywords

        Returns:
            æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        results = []
        for hotspot in hotspots:
            title = hotspot.get("title", "")
            keywords = hotspot.get("keywords", [])
            result = self.check_hotspot(title, keywords)
            result["original_hotspot"] = hotspot
            results.append(result)

        return results

    def get_recent_topics(self, days: int = 7) -> List[str]:
        """
        è·å–è¿‘æœŸå·²å†™çš„ä¸»é¢˜

        Args:
            days: å¤©æ•°

        Returns:
            ä¸»é¢˜åˆ—è¡¨
        """
        cutoff_date = datetime.now() - timedelta(days=days)
        recent_topics = []

        for article in self.articles_cache:
            try:
                article_date = datetime.strptime(article["date"], "%Y-%m-%d")
                if article_date >= cutoff_date:
                    recent_topics.append(article["title"])
            except:
                continue

        return recent_topics

    def generate_report(self, results: List[Dict]) -> str:
        """
        ç”Ÿæˆæ£€æµ‹æŠ¥å‘Š

        Args:
            results: æ‰¹é‡æ£€æµ‹ç»“æœ

        Returns:
            æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        report = []
        report.append("=" * 60)
        report.append("              ğŸ“ å·²å†™æ–‡ç« æ£€æµ‹æŠ¥å‘Š")
        report.append("=" * 60)
        report.append("")

        can_write = []
        need_angle = []
        already_written = []

        for r in results:
            if r["is_written"]:
                already_written.append(r)
            elif r["similar_articles"] and r["similar_articles"][0]["similarity"] >= 0.5:
                need_angle.append(r)
            else:
                can_write.append(r)

        # å¯ä»¥å†™çš„
        if can_write:
            report.append("âœ… å¯ä»¥å†™ï¼ˆæ— ç›¸ä¼¼æ–‡ç« ï¼‰ï¼š")
            report.append("-" * 40)
            for r in can_write:
                report.append(f"  â€¢ {r['hotspot_title']}")
            report.append("")

        # éœ€è¦æ¢è§’åº¦çš„
        if need_angle:
            report.append("âš¡ å»ºè®®æ¢è§’åº¦ï¼ˆæœ‰ç›¸ä¼¼æ–‡ç« ï¼‰ï¼š")
            report.append("-" * 40)
            for r in need_angle:
                similar = r['similar_articles'][0]
                report.append(f"  â€¢ {r['hotspot_title']}")
                report.append(f"    â†³ ç›¸ä¼¼æ–‡ç« ï¼šã€Š{similar['title']}ã€‹({similar['date']}) ç›¸ä¼¼åº¦:{similar['similarity']}")
            report.append("")

        # å·²å†™è¿‡çš„
        if already_written:
            report.append("âš ï¸ å·²å†™è¿‡ï¼ˆä¸å»ºè®®é‡å¤ï¼‰ï¼š")
            report.append("-" * 40)
            for r in already_written:
                similar = r['similar_articles'][0]
                report.append(f"  â€¢ {r['hotspot_title']}")
                report.append(f"    â†³ å·²æœ‰æ–‡ç« ï¼šã€Š{similar['title']}ã€‹({similar['date']}) ç›¸ä¼¼åº¦:{similar['similarity']}")
            report.append("")

        # ç»Ÿè®¡
        report.append("=" * 60)
        report.append(f"ğŸ“Š ç»Ÿè®¡ï¼šå¯å†™ {len(can_write)} | æ¢è§’åº¦ {len(need_angle)} | å·²å†™è¿‡ {len(already_written)}")
        report.append("=" * 60)

        return "\n".join(report)


def main():
    """æµ‹è¯•å…¥å£"""
    detector = WrittenArticleDetector()

    # æµ‹è¯•çƒ­ç‚¹
    test_hotspots = [
        {"title": "Claude 3.5æ›´æ–°ï¼Œä»£ç èƒ½åŠ›å¤§å¹…æå‡", "keywords": ["Claude", "ä»£ç ", "æ›´æ–°"]},
        {"title": "MCPå·¥å…·ä½¿ç”¨æŒ‡å—", "keywords": ["MCP", "å·¥å…·", "æŒ‡å—"]},
        {"title": "Suno v5å‘å¸ƒï¼ŒAIéŸ³ä¹æ–°æ—¶ä»£", "keywords": ["Suno", "AIéŸ³ä¹", "v5"]},
        {"title": "Cursor Proé™ä»·40%", "keywords": ["Cursor", "é™ä»·"]},
        {"title": "æ–°çš„AIè§†é¢‘å·¥å…·å‘å¸ƒ", "keywords": ["AI", "è§†é¢‘", "å·¥å…·"]},
    ]

    results = detector.batch_check(test_hotspots)
    print(detector.generate_report(results))

    # æ‰“å°è¿‘7å¤©å†™è¿‡çš„ä¸»é¢˜
    print("\nğŸ“… è¿‘7å¤©å·²å†™ä¸»é¢˜ï¼š")
    for topic in detector.get_recent_topics(7):
        print(f"  â€¢ {topic}")


if __name__ == "__main__":
    main()
